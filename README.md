# Voice-first Knowledge Base Interview Agent

<parameter name="content"># Voice-first Knowledge Base Interview Agent

A hands-free, voice-driven interview system that builds and maintains a structured knowledge base about your professional experience, skills, and expertise.

## ğŸ¯ Business Outcome

- **Hands-free**: Conduct interviews while driving, walking, or multitasking
- **Structured KB**: Automatically extract and organize facts, timeline, skills, preferences
- **Continuous Growth**: Each session adds to your living knowledge base
- **Multi-system Ready**: Architecture designed for future integrations (Calendar, Email, CRM, Files, etc.)

## ğŸ—ï¸ Architecture

### Tech Stack

- **Frontend**: Next.js 14 (PWA-ready) + React + TailwindCSS
- **Backend**: Fastify + Prisma + TypeScript
- **Database**: PostgreSQL 15 + pgvector (semantic search)
- **Voice**: OpenAI Whisper (STT) + OpenAI TTS
- **LLM**: GPT-4o with Structured Outputs

**Why STT+TTS instead of Realtime API?**
- 10-40x cheaper ($0.006/min vs $0.06-0.24/min)
- Better transcription quality (Whisper specialized model)
- Full control over pipeline (store transcripts, retry logic, cost tracking)
- Acceptable latency for interview use case (2.5-4s total)

See [ARCHITECTURE.md](./ARCHITECTURE.md) for detailed decision rationale.

## ğŸ“¦ Repository Structure

```
.
â”œâ”€â”€ backend/                 # Fastify + Prisma backend
â”‚   â”œâ”€â”€ prisma/             # Database schema + migrations
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ config/         # Environment config
â”‚   â”‚   â”œâ”€â”€ connectors/     # System integration framework
â”‚   â”‚   â”œâ”€â”€ lib/            # Shared utilities
â”‚   â”‚   â”œâ”€â”€ prompts/        # LLM prompts (agent, extractor, consolidator)
â”‚   â”‚   â”œâ”€â”€ routes/         # API routes
â”‚   â”‚   â”œâ”€â”€ services/       # Business logic
â”‚   â”‚   â””â”€â”€ types/          # TypeScript types + schemas
â”‚   â””â”€â”€ Dockerfile
â”‚
â”œâ”€â”€ frontend/               # Next.js PWA frontend
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ app/           # Next.js App Router pages
â”‚   â”‚   â”œâ”€â”€ components/    # React components
â”‚   â”‚   â””â”€â”€ lib/           # API client, utilities
â”‚   â””â”€â”€ Dockerfile
â”‚
â”œâ”€â”€ docker-compose.yml     # Local development setup
â”œâ”€â”€ ARCHITECTURE.md        # Detailed architecture decisions
â””â”€â”€ README.md             # This file
```

## ğŸš€ Quick Start

### Prerequisites

- Node.js 18+
- Docker + Docker Compose
- OpenAI API key

### 1. Clone and Setup

```bash
git clone <repo-url>
cd twin_mike

# Copy environment files
cp backend/.env.example backend/.env
```

### 2. Configure Environment

Edit `backend/.env`:

```env
DATABASE_URL="postgresql://vkb_user:vkb_password@localhost:5432/vkb_dev"
JWT_SECRET="your_secret_at_least_32_characters_long"
OPENAI_API_KEY="sk-..."
```

### 3. Start with Docker Compose

```bash
# Start all services (PostgreSQL + Backend + Frontend)
docker-compose up -d

# Check logs
docker-compose logs -f

# Run database migrations
docker-compose exec backend npm run db:migrate

# Seed example data
docker-compose exec backend npm run db:seed
```

### 4. Access the Application

- **Frontend**: http://localhost:3000
- **Backend API**: http://localhost:3001
- **Health Check**: http://localhost:3001/health
- **Prisma Studio**: `docker-compose exec backend npm run db:studio`

## ğŸ§‘â€ğŸ’» Local Development (without Docker)

### Backend

```bash
cd backend

# Install dependencies
npm install

# Setup database (requires PostgreSQL with pgvector)
npm run db:migrate
npm run db:generate
npm run db:seed

# Start dev server
npm run dev
```

### Frontend

```bash
cd frontend

# Install dependencies
npm install

# Start dev server
npm run dev
```

## ğŸ“š API Documentation

### Authentication

**Magic Link Flow:**

```bash
# 1. Request magic link
POST /auth/start
{
  "email": "user@example.com"
}

# 2. Verify token (from email or dev logs)
POST /auth/verify
{
  "token": "..."
}

# Returns JWT token
{
  "success": true,
  "token": "eyJ...",
  "personId": "...",
  "email": "user@example.com"
}
```

### Sessions

```bash
# Create new session
POST /sessions
Authorization: Bearer <token>
{
  "module": "profile_header"  # optional
}

# Get all sessions
GET /sessions
Authorization: Bearer <token>

# Get session details
GET /sessions/:id
Authorization: Bearer <token>

# Add turn (upload audio)
POST /sessions/:id/turns
Authorization: Bearer <token>
Content-Type: multipart/form-data
Body: audio=<audio_blob>

# Get next agent question
POST /sessions/:id/agent/next
Authorization: Bearer <token>

# Manually trigger extraction
POST /sessions/:id/extract
Authorization: Bearer <token>
```

### Knowledge Base

```bash
# Search KB
GET /kb/search?q=product+management&limit=20
Authorization: Bearer <token>

# Get stats
GET /kb/stats
Authorization: Bearer <token>

# Export KB
GET /kb/export?format=json
GET /kb/export?format=markdown
Authorization: Bearer <token>
```

## ğŸ¤ Using the Voice Interview

1. **Start Session**: Click "Start New Session" on homepage
2. **Push-to-Talk**: Press and hold the microphone button
3. **Speak**: Answer the agent's question
4. **Release**: Let go to send your recording
5. **Listen**: Agent processes your answer and asks next question
6. **Repeat**: Continue the conversation

**Status Indicators:**
- ğŸ¤ **Ready**: Press to start speaking
- â¸ **Listening**: Release to send
- â³ **Processing**: Transcribing + thinking
- ğŸ”Š **Speaking**: Agent is responding

## ğŸ“Š Data Model

### Core Entities

- **Person**: User profile
- **Session**: Interview session (tracks module progress)
- **Turn**: Single exchange (user or agent message)
- **Fact**: Atomic knowledge unit (current role, location, education, etc.)
- **TimelineEntry**: Career history entry
- **Skill**: Skill with proficiency level + evidence
- **Preference**: Work style, communication, leadership preferences
- **Artifact**: Templates, processes, playbooks, links
- **OpenQuestion**: Identified knowledge gaps
- **Summary**: Extraction results per session block

### Interview Modules

1. **profile_header**: Basic facts (role, location, education)
2. **timeline**: Career history with achievements + KPIs
3. **skills**: Technical and soft skills + proficiency
4. **principles**: Leadership style, communication preferences
5. **assets**: Templates, playbooks, processes
6. **stakeholders**: Key relationships
7. **goals**: Career aspirations

Each module runs ~8-12 turns before extraction and consolidation.

## ğŸ”Œ Connector Framework

### Built-in Connectors

1. **MockConnector**: In-memory dummy data for testing
2. **FileConnector**: Local filesystem or S3-compatible storage

### Usage

```typescript
import { connectorManager } from './src/connectors/connector.manager';

// Create record
await connectorManager.execute('mock', 'create', 'calendar_event', {
  title: 'Meeting',
  start: '2024-01-20T10:00:00Z'
});

// Search
const results = await connectorManager.execute('file', 'search', 'documents', 'playbook');

// Health check all
const health = await connectorManager.healthCheckAll();
```

### Adding New Connectors

```typescript
import { BaseConnector } from './base.connector';

export class GoogleCalendarConnector extends BaseConnector {
  id = 'google-calendar';
  name = 'Google Calendar';
  capabilities = ['read', 'write', 'search'];

  async create(entity: string, data: any) {
    // Implement Google Calendar API call
  }

  // ... implement other methods
}

// Register
connectorManager.register(new GoogleCalendarConnector());
```

## ğŸ§ª Testing

### Run Tests

```bash
cd backend
npm test                 # Run all tests
npm run test:watch      # Watch mode
```

### Test Coverage

- Unit tests for services (agent, extractor, consolidator)
- Integration tests for API routes
- Connector tests (mock, file)

## ğŸ“ˆ Cost Tracking

All OpenAI API calls are automatically tracked in `cost_tracking` table.

```sql
SELECT
  service,
  SUM(cost_usd) as total_cost,
  SUM(units) as total_units
FROM cost_tracking
WHERE created_at > NOW() - INTERVAL '30 days'
GROUP BY service;
```

**Estimated Costs (1000 interview minutes/month):**
- Whisper STT: $6
- TTS: $10
- GPT-4o (agent + extraction): $11
- **Total: ~$27/month**

## ğŸ”’ Security & Privacy

- **Audio Storage**: Disabled by default (only transcripts stored)
- **Auth**: Passwordless magic links + JWT
- **Rate Limiting**: 100 req/min per user
- **CORS**: Strict origin whitelist
- **Guardrails**: No politics, sexuality, detailed family/health data

## ğŸŒ Deployment

### Railway (Recommended)

```bash
# Install Railway CLI
npm i -g @railway/cli

# Login
railway login

# Deploy
railway up
```

### Environment Variables (Production)

```env
NODE_ENV=production
DATABASE_URL=<managed_postgres_url>
JWT_SECRET=<strong_random_secret>
OPENAI_API_KEY=<your_key>
CORS_ORIGIN=https://your-domain.com
STORE_AUDIO=false
```

### Docker Production Build

```bash
# Build images
docker-compose -f docker-compose.prod.yml build

# Deploy
docker-compose -f docker-compose.prod.yml up -d
```

## ğŸ› Troubleshooting

### Database Connection Issues

```bash
# Check if PostgreSQL is running
docker-compose ps

# View logs
docker-compose logs postgres

# Reset database
docker-compose down -v
docker-compose up -d
```

### Audio Recording Not Working

- **Chrome/Edge**: Requires HTTPS in production
- **Safari**: May need explicit microphone permission
- **Firefox**: Check `about:preferences` â†’ Privacy â†’ Microphone

### OpenAI API Errors

- Check API key is valid: `backend/.env`
- Verify API key has sufficient credits
- Check rate limits: https://platform.openai.com/account/rate-limits

## ğŸ“ Development Workflow

### Adding a New Module

1. Update `MODULES` in `backend/src/types/index.ts`
2. Add module-specific extraction logic in `extractor.service.ts`
3. Update prompts if needed
4. Add seed data for testing

### Modifying Extraction Schema

1. Update Zod schemas in `backend/src/types/index.ts`
2. Update `system-extractor.txt` prompt with new schema
3. Update consolidator logic if needed
4. Run tests to verify

### Database Migrations

```bash
# Create migration after schema change
cd backend
npm run db:migrate -- --name add_new_field

# Apply migrations
npm run db:migrate:deploy
```

## ğŸ¤ Contributing

1. Create feature branch
2. Make changes
3. Add tests
4. Submit PR

## ğŸ“„ License

MIT

## ğŸ™ Acknowledgments

- OpenAI for Whisper, TTS, and GPT-4o APIs
- Prisma for excellent TypeScript DX
- Fastify for high-performance backend

---

## Sprint 1 âœ… Complete

- âœ… End-to-end voice interview flow
- âœ… STT (Whisper) + TTS pipeline
- âœ… Agent service with module-based interviews
- âœ… Extraction + consolidation
- âœ… Profile Header + Timeline modules
- âœ… Frontend with push-to-talk
- âœ… Connector framework (mock + file)
- âœ… Seed data + example session

## Sprint 2 ğŸš§ Next Steps

1. **Remaining Modules**: skills, principles, assets, stakeholders, goals
2. **Semantic Search**: Implement pgvector embeddings + search API
3. **Consolidation Refinement**: Better deduplication logic, conflict resolution
4. **Export**: Enhanced JSON + Markdown export with formatting
5. **Frontend Enhancements**:
   - Review/edit extracted data screen
   - Session resume
   - KB search interface
6. **Real Connector**: Implement one production connector (e.g., Google Calendar, Notion)
7. **Tests**: Comprehensive test coverage for all services
8. **Performance**: Optimize extraction prompts, reduce token usage
9. **PWA**: Service worker, offline support, install prompt

---

**Built with â¤ï¸ for productive knowledge capture**
